moving_x[k] <- tmp_x[k]
moving_y[k] <- tmp_y[k]
}
delta[iteration] <- delta[iteration]/m
iter_val <- rep(iteration,m)
to_R_tmp <- cbind(iter_val, moving_x, moving_y, rebirth, d2init, d2last)
to_R <- rbind(to_R, to_R_tmp)
} #for(iteration...
version
install.packages(c("foreign", "nlme"))
# Get StackOverflow data
# Get StackOverflow data
get.stack<-function(tok) {
# Must check for XML install, thanks onertipaday!
if (!require(XML)) install.packages('XML')
library(XML)
# Enter a SO tag as character string, and number of tags are returned
#gsub is a grep-like command
#gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE,
#     fixed = FALSE, useBytes = FALSE)
tok<-gsub("(/| )","-",tok)
tok<-gsub("#","%23",tok,fixed=TRUE)
base.stack<-"http://stackoverflow.com/questions/tagged/"
stack.tree<-htmlTreeParse(paste(base.stack,tok,sep=""),useInternalNodes=TRUE)
tag.count<-getNodeSet(stack.tree,"//div[@class='module']/div[@class='summarycount al']")
tag.num<-suppressWarnings(as.numeric(gsub(",","",xmlValue(tag.count[[1]]),fixed=TRUE)))
if(is.na(tag.num)) {
warning(paste("Something went wrong trying to parse '",tok,"'.\nNA returned",sep=""))
}
return(tag.num)
}
get.stack("references")
#get.stack("clojure")
#get.stack("hadoop")
#get.stack("r")
# Get StackOverflow data
get.stack<-function(tok) {
# Must check for XML install, thanks onertipaday!
if (!require(XML)) install.packages('XML')
library(XML)
# Enter a SO tag as character string, and number of tags are returned
#gsub is a grep-like command
#gsub(pattern, replacement, x, ignore.case = FALSE, perl = FALSE,
#     fixed = FALSE, useBytes = FALSE)
tok<-gsub("(/| )","-",tok)
tok<-gsub("#","%23",tok,fixed=TRUE)
base.stack<-"http://stackoverflow.com/questions/tagged/"
stack.tree<-htmlTreeParse(paste(base.stack,tok,sep=""),useInternalNodes=TRUE)
tag.count<-getNodeSet(stack.tree,"//div[@class='module']/div[@class='summarycount al']")
tag.num<-suppressWarnings(as.numeric(gsub(",","",xmlValue(tag.count[[1]]),fixed=TRUE)))
if(is.na(tag.num)) {
warning(paste("Something went wrong trying to parse '",tok,"'.\nNA returned",sep=""))
}
return(tag.num)
}
get.stack("references")
#get.stack("clojure")
#get.stack("hadoop")
#get.stack("r")
installed.packages()
devtools::install_github("nicolewhite/RNeo4j")
library(RNeo4j)
library(RNeo4j)
graph = startGraph("http://localhost:7474/db/data/")
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
View(stock)
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
MATCH (a)-[r]-(b)-[r2]-(c) RETURN a,b,c LIMIT 30
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
devtools::install_github("dataknowledge/visNetwork")
source('~/scripts/rneo4.r')
nodes
source('~/scripts/rneo4.r')
nodes
source('~/scripts/rneo4.r')
nodes
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
nodes
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
MATCH (a1:Person)-[:MATCHPROB]->(a2:Person)
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
nodes
source('~/scripts/rneo4.r')
edges
source('~/scripts/rneo4.r')
nodes
source('~/scripts/rneo4.r')
edges
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
source('~/scripts/rneo4.r')
library(igraph)
library(RNeo4j)
library(visNetwork)
library(jsonlite)
neo4j = startGraph("http://localhost:7474/db/data/")
summary(neo4j)
source('~/scripts/rneo4.r')
N <- 5
g3 <- graph.bipartite.full (N,N)
#Name the vertices A1...AN and B1..BN
V(g3)$name <- c(paste0("A", 1:N), paste0("B", 1:N))
#set the edge weights
set.seed(122)
E(g3)$weight <- sample(10,N^2, replace=T) #use your fWgt function here instead
#verifty if we did things right
str(g3, TRUE)
is.bipartite(g3)
plot (g3,layout=layout.bipartite,
vertex.color=c("green","cyan")[V(g3)$type+1])
mbm <- maximum.bipartite.matching(g3)
sum(E(g3)$weight)
library(igraph)
N <- 5
g3 <- graph.bipartite.full (N,N)
#Name the vertices A1...AN and B1..BN
V(g3)$name <- c(paste0("A", 1:N), paste0("B", 1:N))
#set the edge weights
set.seed(122)
E(g3)$weight <- sample(10,N^2, replace=T) #use your fWgt function here instead
#verifty if we did things right
str(g3, TRUE)
is.bipartite(g3)
plot (g3,layout=layout.bipartite,
vertex.color=c("green","cyan")[V(g3)$type+1])
mbm <- maximum.bipartite.matching(g3)
sum(E(g3)$weight)
N <- 5
g3 <- graph.bipartite.full (N,N)
#Name the vertices A1...AN and B1..BN
library(igraph)
N <- 5
g3 <- graph.full.bipartite (N,N)
#Name the vertices A1...AN and B1..BN
V(g3)$name <- c(paste0("A", 1:N), paste0("B", 1:N))
#set the edge weights
set.seed(122)
E(g3)$weight <- sample(10,N^2, replace=T) #use your fWgt function here instead
#verifty if we did things right
str(g3, TRUE)
is.bipartite(g3)
plot (g3,layout=layout.bipartite,
vertex.color=c("green","cyan")[V(g3)$type+1])
mbm <- maximum.bipartite.matching(g3)
sum(E(g3)$weight)
library(igraph)
N <- 5
g3 <- graph.full.bipartite (N,N)
#Name the vertices A1...AN and B1..BN
V(g3)$name <- c(paste0("A", 1:N), paste0("B", 1:N))
#set the edge weights
set.seed(122)
E(g3)$weight <- sample(10,N^2, replace=T) #use your fWgt function here instead
#verifty if we did things right
str(g3, TRUE)
is.bipartite(g3)
plot (g3,layout=layout.bipartite,
vertex.color=c("green","cyan")[V(g3)$type+1])
mbm <- maximum.bipartite.matching(g3)
sum(E(g3)$weight)
g <- graph.formula( a-b-c-d-e-f )
m1 <- c("b", "a", "d", "c", "f", "e")   # maximal matching
m2 <- c("b", "a", "d", "c", NA, NA)     # non-maximal matching
m3 <- c("b", "c", "d", "c", NA, NA)     # not a matching
is.matching(g, m1)
is.matching(g, m2)
is.matching(g, m3)
is.maximal.matching(g, m1)
is.maximal.matching(g, m2)
is.maximal.matching(g, m3)
library(igraph)
g <- graph.formula( a-b-c-d-e-f )
m1 <- c("b", "a", "d", "c", "f", "e")   # maximal matching
m2 <- c("b", "a", "d", "c", NA, NA)     # non-maximal matching
m3 <- c("b", "c", "d", "c", NA, NA)     # not a matching
is.matching(g, m2)
V(g)$type <- c(FALSE,TRUE)
str(g, v=TRUE)
maximum.bipartite.matching(g)
str(g, v=TRUE)
maximum.bipartite.matching(g)
addProviderTiles("Stamen.Toner", group = "Toner") %>%
library(leaflet)
#df <- read.csv("PRELIM.csv", header=TRUE)
#df <- read.csv("latlong.csv", header=TRUE)
#df <- na.exclude(df)
#df<-df[-2,]
#head(df$long)
#head(df$lat)
twiticon  <- makeIcon(
iconUrl = "http://leafletjs.com/docs/images/leaf-green.png",
iconWidth = 38, iconHeight = 95,
iconAnchorX = 22, iconAnchorY = 94
)
pal <- colorFactor(c("navy", "red"), domain = c(0, 1))
m <- leaflet()   %>% setView(lng = -1.24757, lat = 50.862676, zoom = 9)%>%
#addTiles() %>%  # Add default OpenStreetMap map tiles  set PO15 5RR as default view
addTiles(group = "OSM (default)") %>%
addProviderTiles("Stamen.Toner", group = "Toner") %>%
addProviderTiles("Stamen.TonerLite", group = "Toner Lite") %>%
addProviderTiles("Stamen.Watercolor", group = "Stamen Watercolor") %>%
addProviderTiles("CartoDB.Positron", group = "CartoDB.Positron") %>%
addProviderTiles("CartoDB.DarkMatter", group = "CartoDB.DarkMatter") %>%
# addMarkers( data = df, lat = ~ lat, lng = ~ long , popup=~username ,icon=twiticon, clusterOptions= markerClusterOptions()) %>%
# Layers control
addLayersControl(
baseGroups = c("OSM (default)", "Toner", "Toner Lite","Stamen Watercolor","CartoDB.Positron", "CartoDB.DarkMatter"),
overlayGroups = c("df", "Outline"),
options = layersControlOptions(collapsed = FALSE)
)
library(leaflet)
install.packages("leaflet")
library(leaflet)
twiticon  <- makeIcon(
iconUrl = "http://leafletjs.com/docs/images/leaf-green.png",
iconWidth = 38, iconHeight = 95,
iconAnchorX = 22, iconAnchorY = 94
)
pal <- colorFactor(c("navy", "red"), domain = c(0, 1))
m <- leaflet()   %>% setView(lng = -1.24757, lat = 50.862676, zoom = 9)%>%
#addTiles() %>%  # Add default OpenStreetMap map tiles  set PO15 5RR as default view
addTiles(group = "OSM (default)") %>%
addProviderTiles("Stamen.Toner", group = "Toner") %>%
addProviderTiles("Stamen.TonerLite", group = "Toner Lite") %>%
addProviderTiles("Stamen.Watercolor", group = "Stamen Watercolor") %>%
addProviderTiles("CartoDB.Positron", group = "CartoDB.Positron") %>%
addProviderTiles("CartoDB.DarkMatter", group = "CartoDB.DarkMatter") %>%
# addMarkers( data = df, lat = ~ lat, lng = ~ long , popup=~username ,icon=twiticon, clusterOptions= markerClusterOptions()) %>%
# Layers control
addLayersControl(
baseGroups = c("OSM (default)", "Toner", "Toner Lite","Stamen Watercolor","CartoDB.Positron", "CartoDB.DarkMatter"),
overlayGroups = c("df", "Outline"),
options = layersControlOptions(collapsed = FALSE)
)
m
lines(predict(xy.lm), y, col='blue')
set.seed(2)
x <- 1:100
y <- 20 + 3 * x
e <- rnorm(100, 0, 60)
y <- 20 + 3 * x + e
plot(x,y)
yx.lm <- lm(y ~ x)
lines(x, predict(yx.lm), col='red')
xy.lm <- lm(x ~ y)
lines(predict(xy.lm), y, col='blue')
xyNorm <- cbind(x=x-mean(x), y=y-mean(y))
plot(xyNorm)
#covariance
xyCov <- cov(xyNorm)
eigenValues <- eigen(xyCov)$values
eigenVectors <- eigen(xyCov)$vectors
plot(xyNorm, ylim=c(-200,200), xlim=c(-200,200))
lines(xyNorm[x], eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x])
lines(xyNorm[x], eigenVectors[2,2]/eigenVectors[1,2] * xyNorm[x])
View(edgelist)
View(eigenVectors)
plot(xy)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y))
plot(xy)
lines(x, (eigenVectors[2,1]/eigenVectors[1,1] * xyNorm[x]) + mean(y))
# that looks right. line through the middle as expected
# what if we bring back our other two regressions?
lines(x, predict(yx.lm), col='red')
lines(predict(xy.lm), y, col='blue')
Prior1<-function(n){
mu1<-NULL
mu2<-NULL
lower<-NULL
upper<-NULL
tau<-NULL
for (i in 1:n){
mu1[i]<-runif(1,0,1)
mu2[i]<-runif(1,0,1)
lower[i]<-max(0,(1/mu1[i])+(1/mu2[i])-(1/(mu1[i]*mu2[i])))
upper[i]<-min((1/mu1[i]),(1/mu2[i]))
tau[i]<-runif(1,lower[i],upper[i])
}
return(data.frame(mu1,mu2,tau,lower,upper))
}
Prior1(10)
?igraph_famous
library(igraph)
?igraph_famous
?igraph_ring
library(networkD3)
# load data into a matrix
data <- read.csv(file='Assets-Liabilities.csv', skip=1)
rownames(data) <- data[,1]
data <- as.matrix(data[,-(1:2)])
library(tm)
library(RTextTools)
library(topicmodels)
library(dplyr)
library(stringi)
library(LDAvis)
library(slam)
library(lda)
setwd("/Users/thorosm2002/Dropbox/Rcode/LDA")
Afile<-'holresid.csv'
# load data into a matrix
data <- read.csv(Afile, stringsAsFactors=FALSE)
desc <- data$description
desc <- gsub("'", "", desc)  # remove apostrophes
desc <- gsub("[[:punct:]]", " ", desc)  # replace punctuation with space
desc <- gsub("[[:cntrl:]]", " ", desc)  # replace control characters with space
desc <- gsub("[[:digits:]]+", "", desc) # remove numbers
#desc <- gsub("^[[:space:]]+", "", desc) # remove whitespace at beginning of documents
#desc <- gsub("[[:space:]]+$", "", desc) # remove whitespace at end of documents
desc <- tolower(desc)  #
desc <- gsub("[[:digit:]]+", "", desc) # remove numbers
doc.list <- strsplit(desc, "[[:space:]]+")
# compute the table of terms:
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
# remove terms that are stop words or occur fewer than 5 times:
stop_words <- stopwords("SMART", "")
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
stop_words <- stopwords("SMART")
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
# Compute some statistics related to the data set:
D <- length(documents)  # number of documents
W <- length(vocab)  # number of terms in the vocab
doc.length <- sapply(documents, function(x) sum(x[2, ]))
N <- sum(doc.length)
term.frequency <- as.integer(term.table)
# MCMC and model tuning parameters:
K <- 2
G <- 5000
alpha <- 0.02
eta <- 0.02
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
# fit <-  slda.em(documents, K, vocab, num.e.iterations, num.m.iterations, alpha,
#         eta, annotations, params, variance, logistic = FALSE, lambda = 10,
#         regularise = FALSE, method = "sLDA", trace = 0L, MaxNWts=3000)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
lda_description <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = lda_description$phi,
theta = lda_description$theta,
doc.length = lda_description$doc.length,
vocab = lda_description$vocab,
term.frequency = lda_description$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
lda_description <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = lda_description$phi,
theta = lda_description$theta,
doc.length = lda_description$doc.length,
vocab = lda_description$vocab,
term.frequency = lda_description$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
# MCMC and model tuning parameters:
K <- 3
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
# fit <-  slda.em(documents, K, vocab, num.e.iterations, num.m.iterations, alpha,
#         eta, annotations, params, variance, logistic = FALSE, lambda = 10,
#         regularise = FALSE, method = "sLDA", trace = 0L, MaxNWts=3000)
t2 <- Sys.time()
t2 - t1  # about 24 minutes on laptop
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
lda_description <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
json <- createJSON(phi = lda_description$phi,
theta = lda_description$theta,
doc.length = lda_description$doc.length,
vocab = lda_description$vocab,
term.frequency = lda_description$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
library(tm)
library(RTextTools)
library(topicmodels)
library(lasso2)
setwd("/Users/thorosm2002/Dropbox/Rcode/LDA")
Afile<-'holresid.csv'
# load data into a matrix
data <- read.csv(Afile, stringsAsFactors=FALSE)
descriptions <- data$description
descriptions <- gsub("'", "", descriptions)  # remove apostrophes
descriptions <- gsub("[[:punct:]]", " ", descriptions)  # replace punctuation with space
descriptions <- gsub("[[:cntrl:]]", " ", descriptions)  # replace control characters with space
descriptions <- gsub("[[:digit:]]+", "", descriptions) # remove numbers
#descriptions <- gsub("^[[:space:]]+", "", descriptions) # remove whitespace at beginning of documents
#descriptions <- gsub("[[:space:]]+$", "", descriptions) # remove whitespace at end of documents
descriptions <- tolower(descriptions)  #
dtm <- create_matrix(as.vector(descriptions),
language="english", removeNumbers=TRUE, stemWords=TRUE,
minWordLength = 3,
removePunctuation = TRUE,
weighting=weightTf)
#myctm <- CTM(dtm, k=10 , method = "VEM")
#gr <- build_graph(myctm,lambda = 0.9, and = TRUE)
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *
log2(nDocs(dtm)/col_sums(dtm > 0))
dtm <- dtm[ , term_tfidf >= 0.1]
dtm <- dtm[row_sums(dtm) > 0, ]
lda <- LDA(dtm, 3)
terms(lda,30)
topics(lda)
RHlabel <- data$residential_holiday
RHlabel <- as.factor (data$residential_holiday)
RHlabel <- as.numeric(as.factor (data$residential_holiday))
tops<-topics(lda)
library(tm)
library(RTextTools)
library(topicmodels)
library(lasso2)
setwd("/Users/thorosm2002/Dropbox/Rcode/LDA")
Afile<-'holresid.csv'
# load data into a matrix
data <- read.csv(Afile, stringsAsFactors=FALSE)
descriptions <- data$description
RHlabel <- as.numeric(as.factor (data$residential_holiday))
descriptions <- gsub("'", "", descriptions)  # remove apostrophes
descriptions <- gsub("[[:punct:]]", " ", descriptions)  # replace punctuation with space
descriptions <- gsub("[[:cntrl:]]", " ", descriptions)  # replace control characters with space
descriptions <- gsub("[[:digit:]]+", "", descriptions) # remove numbers
#descriptions <- gsub("^[[:space:]]+", "", descriptions) # remove whitespace at beginning of documents
#descriptions <- gsub("[[:space:]]+$", "", descriptions) # remove whitespace at end of documents
descriptions <- tolower(descriptions)  #
dtm <- create_matrix(as.vector(descriptions),
language="english", removeNumbers=TRUE, stemWords=TRUE,
minWordLength = 3,
removePunctuation = TRUE,
weighting=weightTf)
#myctm <- CTM(dtm, k=10 , method = "VEM")
#gr <- build_graph(myctm,lambda = 0.9, and = TRUE)
# term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *
#   log2(nDocs(dtm)/col_sums(dtm > 0))
# dtm <- dtm[ , term_tfidf >= 0.1]
# dtm <- dtm[row_sums(dtm) > 0, ]
lda <- LDA(dtm, 3)
terms(lda,30)
tops<-topics(lda)
results <- data.frame (RH = RHlabel, intopic=tops)
View(results)
View(results)
results$RH <- factor(results$RH)
results$intopic <- factor(results$intopic)
summary(lm(RH ~ intopic, data = results))
results <- data.frame (RH = RHlabel, intopic=tops)
summary(lm(RH ~ intopic, data = results))
summary(lm(intopic ~ RH, data = results))
summary(lm(intopic ~ factor(RH), data = results))
summary(lm(factor(intopic) ~ factor(RH), data = results))
a<-(lm(intopic ~ factor(RH), data = results))
plot (a)
results <- data.frame (RH = RHlabel, intopic=tops)
gfit = chisq.test(RH, p=intopic, rescale.p=T)
gfit = chisq.test(results$RH, p=results$intopic, rescale.p=T)
gfit
gfit = chisq.test(results$RH, p=results$intopic)
install.packages("FunChisq")
cp.chisq.test(results, method="default")
library(FunChisq)
cp.chisq.test(results, method="default")
chisq.test(results, method="default")
chisq.test(results)
cp.fun.chisq.test(results)
results <- data.frame (RH = RHlabel, intopic=tops)
cp.fun.chisq.test(results)
View(results)
